{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
        "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
        "|<h2>Lecture:</h2>|<h1><b>Byte-pair encoding<b></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
        "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
        "<i>Using the code without the course may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "Q8_-VKZNDmzB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APFQtXa0brAf"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrCWDXUdTS_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the vocabulary"
      ],
      "metadata": {
        "id": "bRwQVVLT6ZNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text with lots of repetitions\n",
        "text = 'like liker love lovely hug hugs hugging hearts'\n",
        "\n",
        "chars = list(set(text))\n",
        "chars.sort() # initial vocab is sorted\n",
        "\n",
        "for l in chars:\n",
        "  print(f'\"{l}\" appears {text.count(l)} times.')"
      ],
      "metadata": {
        "id": "6M7HN5nLTmaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a vocabulary\n",
        "vocab = { word:i for i,word in enumerate(chars) }\n",
        "vocab"
      ],
      "metadata": {
        "id": "mXFIcoLk7Q8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the text needs to be a list, not a string\n",
        "# each element in the list is a token\n",
        "origtext = list(text)\n",
        "print(text)\n",
        "print(origtext)"
      ],
      "metadata": {
        "id": "k70QKgFkBCr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ky9njLdZUgVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now using functions"
      ],
      "metadata": {
        "id": "4Smkdx0MUgRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# > -----------------------------------\n",
        "def get_pair_stats(text2pair):\n",
        "  token_pairs = dict()\n",
        "\n",
        "  # loop over tokens\n",
        "  for i in range(len(text2pair)-1):\n",
        "\n",
        "    # create a pair\n",
        "    pair = text2pair[i] + text2pair[i+1]\n",
        "\n",
        "    # increase pair frequencies\n",
        "    if pair in token_pairs:\n",
        "      token_pairs[pair] += 1\n",
        "    else:\n",
        "      token_pairs[pair] = 1\n",
        "\n",
        "  return token_pairs\n",
        "# ----------------------------------- <\n",
        "\n",
        "\n",
        "\n",
        "# > -----------------------------------\n",
        "def update_vocab(token_pairs,vocab):\n",
        "\n",
        "  # find the most frequent pair\n",
        "  idx = np.argmax(list(token_pairs.values()))\n",
        "  newtok = list(token_pairs.keys())[idx]\n",
        "\n",
        "  # update the vocab\n",
        "  vocab[newtok] = max(vocab.values())+1\n",
        "  return vocab,newtok\n",
        "# ----------------------------------- <\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "def generate_new_token_seq(prevtext,newtoken):\n",
        "\n",
        "  # initialize a new text list\n",
        "  newtext = []\n",
        "\n",
        "  # loop through the list\n",
        "  i = 0\n",
        "  while i<(len(prevtext)-1):\n",
        "\n",
        "    # test whether the pair of this and the following elements match the newly-created pair\n",
        "    if (prevtext[i]+prevtext[i+1]) == newtoken:\n",
        "      newtext.append(newtoken)\n",
        "      i += 2 # skip the next character\n",
        "\n",
        "    # not a pair\n",
        "    else:\n",
        "      newtext.append(prevtext[i])\n",
        "      i += 1 # move to the next character\n",
        "\n",
        "  # add the final token to the list (missing in the video)\n",
        "  if i < len(prevtext):\n",
        "    newtext.append(prevtext[i])\n",
        "\n",
        "  return newtext\n",
        "# ----------------------------------- <"
      ],
      "metadata": {
        "id": "DrX7ThhHcqk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaBv1eDocqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "PQUP3Mu7X501"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-initialize the vocab (again...)\n",
        "vocab = { word:i for i,word in enumerate(chars) }"
      ],
      "metadata": {
        "id": "2CA-ybfLLgil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many tokens in the vocab?\n",
        "vocab_size = 25\n",
        "\n",
        "# make a copy of the original text\n",
        "updated_text = origtext.copy()\n",
        "\n",
        "while len(vocab)<vocab_size:\n",
        "\n",
        "  # get pair statistics\n",
        "  pairs = get_pair_stats(updated_text)\n",
        "\n",
        "  # update the dictionary\n",
        "  vocab,newtoken = update_vocab(pairs,vocab)\n",
        "\n",
        "  # get a new list of tokens\n",
        "  updated_text = generate_new_token_seq(updated_text,newtoken)\n",
        "  print(f'Vocab has {len(vocab)} tokens after adding \"{newtoken}\"')\n"
      ],
      "metadata": {
        "id": "qxIZ6KCcX5x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final tokenized text\n",
        "print(updated_text)"
      ],
      "metadata": {
        "id": "Q3krn5y2-Cnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the final vocab\n",
        "vocab"
      ],
      "metadata": {
        "id": "0avzsroUNvfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zwgt0JTNw0y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}