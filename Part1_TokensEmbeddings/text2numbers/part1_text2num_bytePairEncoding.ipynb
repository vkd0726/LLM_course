{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddingss<h1>|\n",
        "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
        "|<h2>Lecture:</h2>|<h1><b>Byte-pair encoding<b></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
        "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
        "<i>Using the code without the course may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "Q8_-VKZNDmzB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APFQtXa0brAf"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrCWDXUdTS_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the vocabulary"
      ],
      "metadata": {
        "id": "bRwQVVLT6ZNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text with lots of repetitions\n",
        "text = 'like liker love lovely hug hugs hugging hearts'\n",
        "\n",
        "chars = list(set(text))\n",
        "chars.sort() # initial vocab is sorted\n",
        "\n",
        "for l in chars:\n",
        "  print(f'\"{l}\" appears {text.count(l)} times.')"
      ],
      "metadata": {
        "id": "6M7HN5nLTmaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a vocabulary\n",
        "vocab = { word:i for i,word in enumerate(chars) }\n",
        "vocab"
      ],
      "metadata": {
        "id": "mXFIcoLk7Q8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the text needs to be a list, not a string\n",
        "# each element in the list is a token\n",
        "origtext = list(text)\n",
        "print(text)\n",
        "print(origtext)"
      ],
      "metadata": {
        "id": "k70QKgFkBCr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NW4pLnVUgcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find character pairs and merge the most frequent"
      ],
      "metadata": {
        "id": "K_wH9S9WUgZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_pairs = dict()\n",
        "\n",
        "# loop over tokens\n",
        "for i in range(len(origtext)-1):\n",
        "\n",
        "  # create a pair\n",
        "  pair = origtext[i] + origtext[i+1]\n",
        "\n",
        "  # increase pair frequencies\n",
        "  if pair in token_pairs:\n",
        "    token_pairs[pair] += 1\n",
        "  else:\n",
        "    token_pairs[pair] = 1\n",
        "\n",
        "token_pairs"
      ],
      "metadata": {
        "id": "porxIPE4cqrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the most frequent pair\n",
        "mostFreqPair_idx = np.argmax(list(token_pairs.values()))\n",
        "mostFreqPair_char = list(token_pairs.keys())[mostFreqPair_idx]\n",
        "print(f'The most frequent character pair is \"{mostFreqPair_char}\" with {list(token_pairs.values())[mostFreqPair_idx]} appearances')"
      ],
      "metadata": {
        "id": "uFaRRbskcqn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the vocab\n",
        "vocab[mostFreqPair_char] = max(vocab.values())+1\n",
        "vocab"
      ],
      "metadata": {
        "id": "RWWoz2T-FWcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "md3EG369FgmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace the token pair with one token"
      ],
      "metadata": {
        "id": "um8UTajPFght"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a new text list\n",
        "newtext = []\n",
        "\n",
        "# loop through the list\n",
        "i = 0\n",
        "while i<(len(origtext)-1):\n",
        "\n",
        "  # test whether the pair of this and the following elements match the newly-created pair\n",
        "  if (origtext[i]+origtext[i+1]) == mostFreqPair_char:\n",
        "\n",
        "    # append to the new version of the text\n",
        "    newtext.append(mostFreqPair_char)\n",
        "    print(f'added \"{mostFreqPair_char}\"')\n",
        "\n",
        "    # skip the next character\n",
        "    i += 2\n",
        "\n",
        "  # this isn't a merged pair, so add this token to the list\n",
        "  else:\n",
        "    newtext.append(origtext[i])\n",
        "\n",
        "    # move to the next character\n",
        "    i += 1\n",
        "\n",
        "\n",
        "  # add the final token to the list (missing in the video)\n",
        "  if i == (len(origtext)-1):\n",
        "    newtext.append(origtext[i])\n",
        "\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print(f'Original text: {origtext}')\n",
        "print(f'Updated text:  {newtext}')\n",
        "\n",
        "print(f'\\n\\nOriginal text had {len(origtext)} tokens; new text has {len(newtext)} tokens.')"
      ],
      "metadata": {
        "id": "9C4p87b66hMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7oJ4dgs6hPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the most common letter pairs (again!)"
      ],
      "metadata": {
        "id": "axWcmSB76hSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_pairs = dict()\n",
        "\n",
        "# loop over the newtext tokens (not the original!)\n",
        "for i in range(len(newtext)-1):\n",
        "\n",
        "  # create a pair\n",
        "  pair = newtext[i] + newtext[i+1]\n",
        "\n",
        "  # increase pair frequencies\n",
        "  if pair in token_pairs:\n",
        "    token_pairs[pair] += 1\n",
        "  else:\n",
        "    token_pairs[pair] = 1\n",
        "\n",
        "token_pairs"
      ],
      "metadata": {
        "id": "OTmE2Axj6hWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ky9njLdZUgVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now using functions"
      ],
      "metadata": {
        "id": "4Smkdx0MUgRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# > -----------------------------------\n",
        "def get_pair_stats(text2pair):\n",
        "  token_pairs = dict()\n",
        "\n",
        "  # loop over tokens\n",
        "  for i in range(len(text2pair)-1):\n",
        "\n",
        "    # create a pair\n",
        "    pair = text2pair[i] + text2pair[i+1]\n",
        "\n",
        "    # increase pair frequencies\n",
        "    if pair in token_pairs:\n",
        "      token_pairs[pair] += 1\n",
        "    else:\n",
        "      token_pairs[pair] = 1\n",
        "\n",
        "  return token_pairs\n",
        "# ----------------------------------- <\n",
        "\n",
        "\n",
        "\n",
        "# > -----------------------------------\n",
        "def update_vocab(token_pairs,vocab):\n",
        "\n",
        "  # find the most frequent pair\n",
        "  idx = np.argmax(list(token_pairs.values()))\n",
        "  newtok = list(token_pairs.keys())[idx]\n",
        "\n",
        "  # update the vocab\n",
        "  vocab[newtok] = max(vocab.values())+1\n",
        "  return vocab,newtok\n",
        "# ----------------------------------- <\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "def generate_new_token_seq(prevtext,newtoken):\n",
        "\n",
        "  # initialize a new text list\n",
        "  newtext = []\n",
        "\n",
        "  # loop through the list\n",
        "  i = 0\n",
        "  while i<(len(prevtext)-1):\n",
        "\n",
        "    # test whether the pair of this and the following elements match the newly-created pair\n",
        "    if (prevtext[i]+prevtext[i+1]) == newtoken:\n",
        "      newtext.append(newtoken)\n",
        "      i += 2 # skip the next character\n",
        "\n",
        "    # not a pair\n",
        "    else:\n",
        "      newtext.append(prevtext[i])\n",
        "      i += 1 # move to the next character\n",
        "\n",
        "  # add the final token to the list (missing in the video)\n",
        "  if i < len(prevtext):\n",
        "    newtext.append(prevtext[i])\n",
        "\n",
        "  return newtext\n",
        "# ----------------------------------- <"
      ],
      "metadata": {
        "id": "DrX7ThhHcqk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaBv1eDocqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-initialize the vocab\n",
        "vocab = { word:i for i,word in enumerate(chars) }\n",
        "print(f'Vocab has {len(vocab)} tokens.')"
      ],
      "metadata": {
        "id": "jVnWU2qMcqe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4vd-Pn1KjpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## do one iteration\n",
        "\n",
        "# find and count pairs\n",
        "pairs = get_pair_stats(origtext)\n",
        "\n",
        "# update the dictionary\n",
        "vocab,newtoken = update_vocab(pairs,vocab)\n",
        "\n",
        "# get a new list of tokens\n",
        "updated_text = generate_new_token_seq(origtext,newtoken)\n",
        "print(f'Vocab has {len(vocab)} tokens.')"
      ],
      "metadata": {
        "id": "AJJoz6Y8KcPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_text"
      ],
      "metadata": {
        "id": "IfVQp-FkoWmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## do a second iteration\n",
        "pairs = get_pair_stats(updated_text)\n",
        "\n",
        "# update the dictionary\n",
        "vocab,newtoken = update_vocab(pairs,vocab)\n",
        "\n",
        "# get a new list of tokens\n",
        "updated_text = generate_new_token_seq(updated_text,newtoken)\n",
        "print(f'Vocab has {len(vocab)} tokens.')"
      ],
      "metadata": {
        "id": "72G7ZJ1fcqbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "8fzm86CAcqYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zwgt0JTNw0y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}